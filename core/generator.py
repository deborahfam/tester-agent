"""
generator.py
~~~~~~~~~~~~~

Module responsible for interacting with Large Language Models (LLM) through
the OpenRouter API and generating both candidate solutions and test case sets.
This module relies on prompt templates stored in the ``prompts/`` directory
to build requests to the model.  The functions included here are designed to
be easily extensible or adaptable to different LLM providers.

For security and simplicity reasons, API credentials are not directly included;
they must be provided through environment variables or runtime parameters.
If you're testing the project without a real API, you can enable ``mock`` mode
to return example results without making external requests.
"""

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Dict, Iterable, List, Optional

import requests

from .utils import slugify


class LLMClient:
    """Simple client for interacting with the OpenRouter API.

    This class encapsulates HTTP request creation.  If the ``mock`` parameter
    is enabled in the constructor, responses are simulated by returning
    predefined examples.  You can extend this client to support different
    models or authentication systems.
    """

    def __init__(self, api_key: Optional[str] = None, model: str = "mistralai/mixtral-8x7b", mock: bool = False) -> None:
        self.api_key = api_key or os.environ.get("OPENROUTER_API_KEY", "")
        self.model = model
        self.mock = mock
        self.endpoint = "https://openrouter.ai/api/v1/chat/completions"

    def _post(self, data: Dict) -> Dict:
        headers = {"Authorization": f"Bearer {self.api_key}"}
        response = requests.post(self.endpoint, json=data, headers=headers)
        response.raise_for_status()
        return response.json()

    def query(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """Sends a prompt to the model and returns the response as text.

        :param prompt: User text describing the task.
        :param system_prompt: Optional, system text that conditions the
            model's behavior.
        :return: Text generated by the model.
        """
        if self.mock:
            # Demo response for testing without real API
            return "Example LLM response."
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": 2048,
            "temperature": 0.3,
        }
        result = self._post(payload)
        # OpenRouter response structure follows OpenAI format
        return result.get("choices", [{}])[0].get("message", {}).get("content", "")


def load_prompt(template_name: str) -> str:
    """Loads a prompt template from the `prompts/` directory.

    :param template_name: Template file name, for example
      ``"generate_solution.txt"``.
    :return: Textual content of the template.
    :raises FileNotFoundError: If the specified file does not exist.
    """
    prompts_dir = Path(__file__).resolve().parent.parent / "prompts"
    template_path = prompts_dir / template_name
    return template_path.read_text(encoding="utf-8")


def generate_solutions(problem_data: Dict, template_code: str, n: int = 2, client: Optional[LLMClient] = None) -> List[str]:
    """Generates `n` candidate solutions for an exercise.

    Builds a prompt from the information extracted from the exercise and a
    blank code template.  Sends this prompt to the LLM to obtain different
    implementations of the requested function.

    :param problem_data: Dictionary with fields like ``statement``, ``input``
      and ``output`` obtained from the extractor.
    :param template_code: Base code (for example an empty ``solve`` function).
    :param n: Number of solutions to generate.
    :param client: Instance of :class:`LLMClient`.  If ``None``, a default
      instance will be constructed.
    :return: List of strings with Python code generated by the model.
    """
    client = client or LLMClient()
    prompt_template = load_prompt("generate_solution.txt")
    prompt_text = prompt_template.format(
        statement=problem_data.get("statement", ""),
        input=problem_data.get("input", ""),
        output=problem_data.get("output", ""),
        constraints=problem_data.get("constraints", ""),
        examples=problem_data.get("examples", ""),
        template_code=template_code,
    )
    solutions: List[str] = []
    for _ in range(max(1, n)):
        response = client.query(prompt_text)
        solutions.append(response.strip())
    return solutions


def generate_test_cases(problem_data: Dict, candidate_code: str, max_tests: int = 10, client: Optional[LLMClient] = None) -> List[Dict[str, str]]:
    """Requests the LLM to propose test cases based on a statement.

    Since test case generation depends on both the statement and a specific
    implementation, the candidate code is included in the prompt so the model
    can analyze corner cases and constraints.  The prompt template must
    define the expected response format (for example, a JSON with pairs of
    inputs and expected outputs).

    :param problem_data: Dictionary with problem information.
    :param candidate_code: A candidate implementation of the function.
    :param max_tests: Maximum number of test cases to request.
    :param client: LLM client instance.
    :return: List of test cases as dictionaries.
    """
    client = client or LLMClient()
    prompt_template = load_prompt("generate_tests.txt")
    prompt_text = prompt_template.format(
        statement=problem_data.get("statement", ""),
        input=problem_data.get("input", ""),
        output=problem_data.get("output", ""),
        constraints=problem_data.get("constraints", ""),
        examples=problem_data.get("examples", ""),
        code=candidate_code,
        max_tests=max_tests,
    )
    response = client.query(prompt_text)
    # Response is expected to be valid JSON.  We try to parse it.
    try:
        tests = json.loads(response)
        if isinstance(tests, list):
            return tests
    except json.JSONDecodeError:
        pass
    # If it's not valid JSON, return an empty list and warn in the log
    return []